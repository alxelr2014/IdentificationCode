\documentclass{article}
\usepackage[a4paper,bindingoffset=0.2in,%
left=0.75in,right=0.75in,top=1in,bottom=1in,%
footskip=.25in]{geometry}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage[ruled,noline]{algorithm2e}
\usepackage{codestyles}
\usepackage{listings}
\usepackage{hyperref}

\DeclarePairedDelimiter{\ceilp}{\lceil}{\rceil}
\DeclarePairedDelimiter{\floorp}{\lfloor}{\rfloor}
\DeclarePairedDelimiter{\roundp}{\lfloor}{\rceil}

\newcommand{\floor}[1]{\floorp{#1}}
\newcommand{\ceil}[1]{\ceilp{#1}}
\newcommand{\round}[1]{\roundp{#1}}

\newtheorem{definition}{Definition}
\newtheorem{lemma}{Lemma}

\newcommand{\abs}[1]{\left| #1 \right|}

\begin{document}
\section{Week 1}
\subsection{Multiway paper}
This week, I mainly worked on the Ahlswede's \cite{multiway} paper. Here is a summary of what I did:
\begin{itemize}
	\item The ``\(\sqrt{n}\) trick'' is described in \cite{idfeedback}. The idea behind the section F's 3 step algorithm is also described in previous works of Ahlswede. I will take a look at the referenced article next week.
	\item I think the section B. can be described more intuitively as follows:
	      \begin{itemize}
		      \item \(\Omega\) is the set of \textit{terminals} which are basically the communication devices.
		      \item \(\Gamma\) is the set of \textit{messengers} which can be viewd as a pair of transmitter and receiver. For each messenger \(\gamma\), \(\mathcal{N}_{\gamma}\) is the set of messages that the transmitter and receiver communicate with.
		      \item For each terminal \(\omega \in \Omega\), the set of transmitters on that terminal is denoted by \(\mathcal{A}_{\omega}\).
		      \item For each terminal \(\omega \in \Omega\), the set of receiver on that terminal is denoted by \(\mathcal{B}_{\omega}\). --- there is a typo in the definition given in the paper, the last \(\mathcal{B}_{\omega}\) should be changed to \(\omega\).
		      \item For each terminal \(\omega \in \Omega\), the set of available feedback lines is denoted by \(\Phi_{\omega}\). --- there is a typo here too, \(\Phi_{\omega} \subset \Omega\) and not \(\Gamma\).
		      \item The channel \(W\) is discrete and memoryless.
	      \end{itemize}
	      the given assumption can also be interpreted as follows
	      \begin{itemize}
		      \item \(\mathcal{A}_{\omega} \cap \mathcal{B}_{\omega} = \emptyset\):
		            because otherwise, the transmitter and receiver would be placed on the same terminal which makes communication via channel unnecessary.

		            \(\cup_{\omega \in \Omega} \mathcal{A}_{\omega} = \cup_{\omega \in \Omega } \mathcal{B}_{\omega} = \Gamma\): we can assume that each transmitter/receiver is placed only on one terminal.
		      \item If \(\abs{\mathcal{X}_{\omega}} = \abs{\mathcal{Y}_{\omega}} = 1\), then terminal can not transmit or receive any information.
		      \item If \(\abs{\mathcal{X}_{\omega}}\) then the terminal can not send information, hence no transmitter should be placed on it. Similarly for receiving.
		      \item I did not fully understand what is logic behind \(A_4\) but I guess that is related to relay channels, since the relays do not send or decode data.
		      \item \(\omega \in \Phi_{\omega}\) every terminal should know what it received.
		      \item If \(\gamma \in \mathcal{A}_{\omega} \cap \mathcal{B}_{\omega'}\), then the transmitter of \(\gamma\) is on \(\omega\) and its receiver is on \(\omega'\). Then, all the information available at \(\omega'\) is feedbacked to \(\omega\), i.e. \(\Phi_{\omega'} \subset \Phi_{\omega}\).
		      \item Passive decoders do not need to transmit anything.
	      \end{itemize}
	\item On section C:
	      \begin{itemize}
		      \item Randomized feedback is not explained, I dont see what makes them different than the stochastic feedback defined later.
		      \item ``concatentation of strategies'' after equation (1.6) is ambiguous.
		      \item Derivation of equation (1.8) might be something like the following, but I am not sure as the definition are not formal.
		            \begin{align*}
			            \mu(\mathcal{F}_{m + n}) & = \max_{f^{n + m} \in \mathcal{F}_{n + m}} H(Y^{n + m}(f^{n+ m}))                              \\
			                                     & \geq  \max_{f^{n + m} \in \mathcal{F}_n \times \mathcal{F}_m } H(Y^{n + m}(f^{n+ m}))          \\
			                                     & = \max_{f^{n} \in \mathcal{F}_n} \max_{f^m \in \mathcal{F}_m } H(Y^{n }(f^{n}) , Y^m(f^m))     \\
			                                     & = \max_{f^{n} \in \mathcal{F}_n} \max_{f^m \in \mathcal{F}_m } H(Y^{n }(f^{n}) )+ H( Y^m(f^m)) \\
			                                     & = \mu(\mathcal{F}_n) + \mu(\mathcal{F}_m)
		            \end{align*}
		            I used independence in the second and the third line.
		      \item Given the above inequality, \(\mu(\mathcal{F}_n) \geq n \mu(\mathcal{F}_1)\) and therefore
		            \begin{equation*}
			            \mu((\mathcal{F}_n)_{n = 1}^{\infty}) = \lim_{n \to \infty} \dfrac{1}{n \mu(\mathcal{F}_{n})} \leq \lim_{n \to \infty} \dfrac{1}{n^2 \mu(\mathcal{F}_{1})} =? 0
		            \end{equation*}
		            and since \(\mu \geq 0\), then \(\mu = 0\)??!.
		      \item I skimmed the remaining sections. I am not sure how the mystery number \(\mu\) is related to the 3-step algorithm.
	      \end{itemize}
\end{itemize}
\subsection{Randomized prime}
I also worked on the randomized prime generation idea. Consider the following algorithm
\begin{algorithm}
	\DontPrintSemicolon
	\SetKwInOut{Input}{input}\SetKwInOut{Output}{output}
	\Input{\(n,t\)}
	\Output{A uniform \(n\)-bit prime}
	\For{\(i = 1 \to t\)}{
	\(p \gets \{0,1\}^{n}\)\;
	\If{\(p\) is prime}{
		\Return{\(p\)}
	}
	}
	\Return{\(\perp\)}
	\caption{Generating random primes}
\end{algorithm}
Instead of checking that \(p\) is a prime, we can check if \(p\) passes the Miller-Rabin test or not, which is more efficient -- running in \(O(n^3)\) rather than in \(O(2^n)\) for a simple primality test, or \(\tilde{O}{n^6}\) for AKS primality test. By applying the Miller-Rabin test multiple times, the mathbb{P}ability of error (a composite number passes the test) decreases rapidly. Moreover, by letting \(n\) to be large enough (an asymptotic formula can be derived from the Prime Number Theorem), we can be sure that \(\pi_K\) can be represented by \(n\)-bits, and therefore our analysis for the 3-step algorithms remains unchanged.

For the next week I am going to do a more thorough derivation of the above idea and implement it.
\section{Week 2}
Add the randomized prime generation and a testing python code to plot the error rate.
\section{Week 3}
\subsection{The simulation code}
\subsubsection{\texttt{Channel} class}
\lstinputlisting[style = cpp-code, firstline = 9,linewidth = \textwidth]{../include/channel.h}
The \texttt{channel} class models a discrete memoryless channel.  The \texttt{ChannelFunc} is a function that takes an input character -- the characters are modeled as indicies-- and returns an output character. This function might be randomized as well. In fact the channel's transition matrix \(W\) should be implemented in \texttt{ChannelFunc} and be given as input to the constructor. It was wiser to let indicies start from \(0\) and I will refactor the code.

The \texttt{transmit} method simply calls the \texttt{ChaanelFunc f} on the given symbol.

\subsubsection{\texttt{Identification} class}
\lstinputlisting[style = cpp-code, firstline = 11,linewidth = \textwidth]{../include/identification.h}
This class models an identification code for a given channel \texttt{C}. The construction of the codes is done by \texttt{constructID\_Code} which takes a \texttt{construction\_method} to do the job. The \texttt{encoder} is a function that takes a message and encodes to a block of channel's inpute character.  The \texttt{decoder} returns \textbf{the number of identified messages}. Under a uniform distribution on messages, the second error rate -- false identification -- is equal to the number of identified messages divided by the number of messages \(N\). I have not implemented the \texttt{getFirstKindError} and \texttt{getSecondKindError} yet.

There is a similar \texttt{transmission} class which is supposed to model transmission codes. I added this class because of the constructions in \cite{idfeedback} and \cite{verdu} that use transmission codes. However, I have only implemented the 3-step algorithm so far.
\subsubsection{\texttt{Codes} class}
\lstinputlisting[style = cpp-code, firstline = 14,linewidth = \textwidth]{../include/codes.h}
For now it only implements the 3-step algorithm.
\subsubsection{\texttt{Simulate} class}
\lstinputlisting[style = cpp-code, firstline = 14,linewidth = \textwidth]{../include/simulate.h}
It takes as input a \texttt{Channel C} and a identification code constructor \texttt{construction\_method} then, it simulates the transmission of a message over channel. For now, it outputs the second error rate of the Identification code. 
\subsubsection{main}
\lstinputlisting[style = cpp-code, firstline = 11,linewidth = \textwidth]{../src/main.cpp}
The \texttt{main} method takes some optional inputs 
\begin{enumerate}
	\item Address of a file for logging.
	\item The parameter \(N\), the number messages.
	\item The parameter \(n\), the block size -- for 3-step algorithm is not important.
	\item The parameter \(m\), the number of simulations.
	\item A random seed for initializing the random generator.
\end{enumerate}
\subsection{Random Prime Generation}
The pseudocode of our prime generation algorithm is 
\begin{algorithm}
	\DontPrintSemicolon
	\SetKwInOut{Input}{input}\SetKwInOut{Output}{output}
	\Input{positive integers \(m,s,k\)}
	\Output{A uniformly chosen odd prime less than or equal to \(m\)}
	\For{\(i = 1 \to s\)}{
	\(n \gets \{3,5,\dots, m\}\)\;
	\If{Miller\_Rabin\((n,k)\) is PRIME}{
		\Return{\(n\)}
	}
	}
	\Return{\(23\)}
	\caption{pseudocode}
\end{algorithm}


Instead of returning \(\perp\) when all of the randomly chosen numbers are COMPOSITE, it returns 23. Therefore, the probability of finding no prime number is 
\begin{align*}
	\mathbb{P}\left\{\perp\right\} &= \mathbb{P}\left\{M\_R(n_1,k) = \mathrm{COMPOSITE}, \dots , M\_R(n_s,k) = \mathrm{COMPOSITE}\right\}\\
	&= \prod_{i= 1}^s \mathbb{P}\left\{M\_R(n_i,k)= \mathrm{COMPOSITE}\right\}\\
	&= \prod_{i= 1}^s \mathbb{P}\left\{n_i \text{ is composite}\right\}\\
	&= \prod_{i= 1}^s \left(1 -\mathbb{P}\left\{n_i \text{ is prime}\right\}\right)\\
	&\approx \prod_{i= 1}^s \left(1 - \frac{1}{\ln m}\right)\\
	&= \left(1 - \frac{1}{\ln m}\right)^s
\end{align*}
where we used the fact that \(\frac{\pi(m)}{m} \approx \frac{1}{\ln m}\) asymptotically by prime number theorem \cite{apostol}. From Theorem 31.39 of \cite{clrs} and its following analysis, for moderate values of \(s \approx 3\), the probability of error is negligible. That is, if the algorithm returns a number -- not \(\perp\)-- then it is most likely a prime. Then, the number of iteration to get a prime number is about 
\begin{align*}
	&\mathbb{P}\left\{\perp\right\}  \leq \frac{1}{2}\\
	\implies& s \lg\left(1 - \frac{1}{\ln m}\right) \leq -1 \\
	\implies& s  \geq \dfrac{-1}{\lg\left(1 - \frac{1}{\ln m}\right)}\\
	\implies& s  \geq \dfrac{-\ln 2}{\ln\left(1 - \frac{1}{\ln m}\right)} 
	\intertext{note that \(\ln(1 - \frac{1}{x}) \approx -\frac{1}{x}\) for large enough \(x\)}
	\implies& s  \geq \ln m\ln 2 = \ln^2 2 \lg m
\end{align*}
which means that \(s\) is in the order of number of bits of \(m\).
\section{Week 4}
Identification is a communication paradigm introduced by Ahlswede \cite{idfeedback}. In identification schemes, in essence, the receiver wants to know whether a certain message has been send or not. This is in contrast to the Shannon's transmission paradigm where the receiver wants to know the content of the message. 

More formally, the send and receiver both have the message set \(\mathcal{M}\) and the receiver is interested in message \(m \in \mathcal{M}\). Ofcourse, when the sender knows \(m\), he can send a bit to indicate that he intends to send \(m\) or not. We may then assume that sender does not know \(m\). 

This problem can be trivially addressed by transmission codes, the receiver decodes the received code to \(\hat{m}\) and then decides if \(\hat{m} = m\). However, the  Ahlswede's identification codes require exponentially shorter blocklength to identify the same number of messages. This improvement is achieved mainly by relaxing the condition that the decoding sets need be disjoint. By allowing the decoding sets to have slight overlap, Ahlswede \cite{idfeedback} has shown that there exists coding schemes that can identify \(2^{2^{nC}}\) messages where \(C\) is the Shannon capacity of the DMC channel.

There are two kinds of errors associated with an identification scheme. The first kind happens when the sender sends \(m\) but the receiver fails to identify it and hence \textit{misses} the identification. The second kind happens when the sender send \(m' \neq m\) and the receiver \textit{falsely} identifies \(m\) instead.

\begin{definition}[Identification code]
	An identification code \(n, N,\lambda_1, \lambda_2\) for a DMC channel \(\mathcal{W^n}(\mathcal{X}^n | \mathcal{Y}^n)\) is a set \(\{ Q( \cdot |i), \mathcal{D}_i \}_{i \in [N]}\) where \(Q(\cdot | i)\) is a distribution over \(\mathcal{X}^n\) to that encodes \(i\) -- for determinstic encoder \(Q(x_i | i) = 1\) for some \(x_i \in \mathcal{X}^n\), and \(\mathcal{D}_i \subset \mathcal{Y}\) is the decoding set of \(i\). The first and second kind errors are bounded by \(\lambda_1\) and \(\lambda_2\), respectively.
	\begin{align*}
		&\sum_{x^n \in \mathcal{X}^n} Q(x^n | i) W^n(\mathcal{D}_i^c | x^n) \leq \lambda_1 \\
		&\sum_{x^n \in \mathcal{X}^n} Q(x^n | j) W^n(\mathcal{D}_i | x^n) \leq \lambda_2 \\
	\end{align*}
\end{definition}


\subsection{Prime Number Generator}
Prime number generators are algorithms that generate primes (I guess :))) well technically the name is self-explanatory). There are multitude of PNG, each might be desirable given the requirements of the problem. Typically we may consider two types of PNG; prime sieves and primality tests. In a prime sieve we look for all primes in a given interval \([m,n]\) where usually \(m = 1\). One of the simplest --but not the most efficient-- sieves is the sieve of Eratosthenes. 
\begin{algorithm}
	\DontPrintSemicolon
	\SetKwInOut{Input}{input}\SetKwInOut{Output}{output}
	\Input{positive integer \(n\)}
	\Output{All prime numbers less than or equal to \(n\)}
	create a bitset \(b\) of size \(n\) set initially to all true\;
	\(b[1] =\) false\;
	\(P = \emptyset\)\;
	\For{\(i = 2, \dots, n\)}{
	\If{\(b[i]\) is true}{
		\(P = P \cup \{i\}\)\;
		\For{\(j = 2i, \dots, \floor{\frac{n}{i}}i \)}{
			\(b[j] =\) false\;
		}
	}
	}
	\Return{\(P\)}
	\caption{sieve of Eratosthenes}
\end{algorithm}
Prime sieves are not very efficient, even if there was no computation, just outputing the primes less than \(n\) is of order \(O(n/\ln n)\), which makes these algorithms sub-exponential in size of \(n\). 

However, in most application we look for some large primes and not all primes. One  strategy to generate such primes is to pick a random number -- perhaps from a desirable distribution-- and then check whether that number is a prime. Therefore, we need an efficient primality test algorithm. There are determinstic and randomized polynomial time primality tests. However, currently the best determinstic algorithms are much slower than randomized ones -- higher polynomial degree-- and at the same time, the randomized algorithms can be made very accurate. As a result, for most applications a randomized test is used. A list of basic primality test algorithms can be found in \cite{das}.

\subsection{3-Step Algorithm}
In essence the 3-Step algorithm does the following:
\begin{enumerate}
	\item The sender uniformly chooses two indicies \(k,l\) from \([K],[K']\) respectively.
	\item Given any message \(m \in [M]\), the sender send \(\phi_l(\phi_k(m))\) to the receiver. 
\end{enumerate} 
I think if we send \(\pi_k\) and \(\pi_l\) instead of the indicies can signficantly improve the efficiency of the receiver without compromising the rate. Note that, \(\pi_k = O(k \ln k)\) -- the constant term is relatively small \(C \leq 30\) for all integers and \(C \leq 15\) for \(k \geq 200\). Therefore, the asymptotic transmission cost of this modified scheme is 
\begin{align}
	n &= \ceil{\lg \pi_K} + 2\ceil{\lg \pi_{K'}} \\
	&= O(\lg K + \lg\lg K) + O(\lg K' + \lg \lg K') \\
	&= O(\lg K) + O(\lg K') \sim \alpha [1 + o(1)] \lg \lg M
\end{align}
which is the same as before. The benefit of this modification is that the receiver does not need to compute \(\pi_k\) and \(\pi_l\) from \(k\) and \(l\) respectively -- as far as I know only a prime sieve can do this reliably, however as explained above, the sieves are subexponential. 

I also came up with a new error analysis which \underline{I think} is better than the Ahlswede's. First consider the following lemmas
\begin{lemma}
	For any \(x \geq 2\),
	\begin{equation}
		\frac{1}{x}\sum_{p \leq x} \frac{1}{p} = \dfrac{\lg \lg x}{x} + O(\frac{1}{x})
	\end{equation}
	The approximate term can be expanded in terms of Mertens' constant.
\end{lemma}

\begin{lemma}
	Let \(m,m' \gets [M]\) and \(k \gets [K]\) be a uniform random variable independent of \(m,m'\). Then,
	\begin{equation*}
		\mathbb{P}\left( \phi_k(m) = \phi_k(m')  \middle| m \neq m'  \right) \leq \dfrac{\lg \lg K}{K} + O(\frac{1}{K})
	\end{equation*}
\end{lemma}

\begin{proof}
	We easily have 
	\begin{align*}
		\mathbb{P}\left(  \phi_k(m) = \phi_k(m') \middle| m \neq  m'  \right) &= \sum_{\hat{k} = 1}^K \mathbb{P}\left(  \phi_k(m) = \phi_k(m') \middle| m \neq m', k = \hat{k}   \right) \mathbb{P}\left( k = \hat{k} \middle| m \neq m'  \right) \\
		&= \dfrac{1}{K} \sum_{\hat{k} = 1}^K \mathbb{P}\left(  \phi_k(m) = \phi_k(m')\middle| m \neq m', k = \hat{k}   \right) \\
		&\leq \dfrac{1}{K} \sum_{\hat{k} = 1}^K  \dfrac{1}{\pi_{\hat{k}}} \\
		&= \dfrac{\lg \lg K}{K} + O(\frac{1}{K})
	\end{align*}
\end{proof}
Now by following the error analysis of Ahlswede we get 
\begin{equation*}
	\mathbb{P}\left( \phi_l(\phi_k(m)) = \phi_l(\phi_k(m'))\middle| m \neq m'  \right) \leq \dfrac{\lg \lg K}{K} + \dfrac{\lg \lg K'}{K'}
\end{equation*}
and we have 
\begin{align*}
	\dfrac{\lg \lg K}{K} = \dfrac{\lg \alpha + \lg \lg M}{(\lg M)^{\alpha}} \leq \dfrac{1}{(\lg M)^{\alpha - 1}}
\end{align*}
I came up with this when I was working on the error analysis of our code, where we used the Miller-Rabin to generate prime. I am currently working on the distribution of the generate numbers and I will add it as soon as I become confident.
\bibliographystyle{plain} % We choose the "plain" reference style
\bibliography{references}
\end{document}